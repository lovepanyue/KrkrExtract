{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dataset.py",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMRFs/1l1C4f4ws2guYUxGd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lovepanyue/KrkrExtract/blob/master/dataset_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qOXw7Wov-qu1"
      },
      "outputs": [],
      "source": [
        "import multiprocessing\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def make_anime_dataset(img_paths, batch_size, resize=64, drop_remainder=True, shuffle=True, repeat=1):\n",
        "\n",
        "    # @tf.function\n",
        "    def _map_fn(img):\n",
        "        img = tf.image.resize(img, [resize, resize])\n",
        "        # img = tf.image.random_crop(img,[resize, resize])\n",
        "        # img = tf.image.random_flip_left_right(img)\n",
        "        # img = tf.image.random_flip_up_down(img)\n",
        "        img = tf.clip_by_value(img, 0, 255)\n",
        "        img = img / 127.5 - 1 #-1~1\n",
        "        return img\n",
        "\n",
        "    dataset = disk_image_batch_dataset(img_paths,\n",
        "                                          batch_size,\n",
        "                                          drop_remainder=drop_remainder,\n",
        "                                          map_fn=_map_fn,\n",
        "                                          shuffle=shuffle,\n",
        "                                          repeat=repeat)\n",
        "    img_shape = (resize, resize, 3)\n",
        "    len_dataset = len(img_paths) // batch_size\n",
        "\n",
        "    return dataset, img_shape, len_dataset\n",
        "\n",
        "\n",
        "def batch_dataset(dataset,\n",
        "                  batch_size,\n",
        "                  drop_remainder=True,\n",
        "                  n_prefetch_batch=1,\n",
        "                  filter_fn=None,\n",
        "                  map_fn=None,\n",
        "                  n_map_threads=None,\n",
        "                  filter_after_map=False,\n",
        "                  shuffle=True,\n",
        "                  shuffle_buffer_size=None,\n",
        "                  repeat=None):\n",
        "    # set defaults\n",
        "    if n_map_threads is None:\n",
        "        n_map_threads = multiprocessing.cpu_count()\n",
        "    if shuffle and shuffle_buffer_size is None:\n",
        "        shuffle_buffer_size = max(batch_size * 128, 2048)  # set the minimum buffer size as 2048\n",
        "\n",
        "    # [*] it is efficient to conduct `shuffle` before `map`/`filter` because `map`/`filter` is sometimes costly\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(shuffle_buffer_size)\n",
        "\n",
        "    if not filter_after_map:\n",
        "        if filter_fn:\n",
        "            dataset = dataset.filter(filter_fn)\n",
        "\n",
        "        if map_fn:\n",
        "            dataset = dataset.map(map_fn, num_parallel_calls=n_map_threads)\n",
        "\n",
        "    else:  # [*] this is slower\n",
        "        if map_fn:\n",
        "            dataset = dataset.map(map_fn, num_parallel_calls=n_map_threads)\n",
        "\n",
        "        if filter_fn:\n",
        "            dataset = dataset.filter(filter_fn)\n",
        "\n",
        "    dataset = dataset.batch(batch_size, drop_remainder=drop_remainder)\n",
        "\n",
        "    dataset = dataset.repeat(repeat).prefetch(n_prefetch_batch)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def memory_data_batch_dataset(memory_data,\n",
        "                              batch_size,\n",
        "                              drop_remainder=True,\n",
        "                              n_prefetch_batch=1,\n",
        "                              filter_fn=None,\n",
        "                              map_fn=None,\n",
        "                              n_map_threads=None,\n",
        "                              filter_after_map=False,\n",
        "                              shuffle=True,\n",
        "                              shuffle_buffer_size=None,\n",
        "                              repeat=None):\n",
        "    \"\"\"Batch dataset of memory data.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    memory_data : nested structure of tensors/ndarrays/lists\n",
        "\n",
        "    \"\"\"\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(memory_data)\n",
        "    dataset = batch_dataset(dataset,\n",
        "                            batch_size,\n",
        "                            drop_remainder=drop_remainder,\n",
        "                            n_prefetch_batch=n_prefetch_batch,\n",
        "                            filter_fn=filter_fn,\n",
        "                            map_fn=map_fn,\n",
        "                            n_map_threads=n_map_threads,\n",
        "                            filter_after_map=filter_after_map,\n",
        "                            shuffle=shuffle,\n",
        "                            shuffle_buffer_size=shuffle_buffer_size,\n",
        "                            repeat=repeat)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def disk_image_batch_dataset(img_paths,\n",
        "                             batch_size,\n",
        "                             labels=None,\n",
        "                             drop_remainder=True,\n",
        "                             n_prefetch_batch=1,\n",
        "                             filter_fn=None,\n",
        "                             map_fn=None,\n",
        "                             n_map_threads=None,\n",
        "                             filter_after_map=False,\n",
        "                             shuffle=True,\n",
        "                             shuffle_buffer_size=None,\n",
        "                             repeat=None):\n",
        "    \"\"\"Batch dataset of disk image for PNG and JPEG.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        img_paths : 1d-tensor/ndarray/list of str\n",
        "        labels : nested structure of tensors/ndarrays/lists\n",
        "\n",
        "    \"\"\"\n",
        "    if labels is None:\n",
        "        memory_data = img_paths\n",
        "    else:\n",
        "        memory_data = (img_paths, labels)\n",
        "\n",
        "    def parse_fn(path, *label):\n",
        "        img = tf.io.read_file(path)\n",
        "        img = tf.image.decode_jpeg(img, channels=3)  # fix channels to 3\n",
        "        return (img,) + label\n",
        "\n",
        "    if map_fn:  # fuse `map_fn` and `parse_fn`\n",
        "        def map_fn_(*args):\n",
        "            return map_fn(*parse_fn(*args))\n",
        "    else:\n",
        "        map_fn_ = parse_fn\n",
        "\n",
        "    dataset = memory_data_batch_dataset(memory_data,\n",
        "                                        batch_size,\n",
        "                                        drop_remainder=drop_remainder,\n",
        "                                        n_prefetch_batch=n_prefetch_batch,\n",
        "                                        filter_fn=filter_fn,\n",
        "                                        map_fn=map_fn_,\n",
        "                                        n_map_threads=n_map_threads,\n",
        "                                        filter_after_map=filter_after_map,\n",
        "                                        shuffle=shuffle,\n",
        "                                        shuffle_buffer_size=shuffle_buffer_size,\n",
        "                                        repeat=repeat)\n",
        "\n",
        "    return dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fSrDGhNiCjIq"
      }
    }
  ]
}